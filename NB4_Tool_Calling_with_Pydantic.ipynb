{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f4e903a",
   "metadata": {},
   "source": [
    "# Tool Calling with Pydantic Models and OpenAI\n",
    "\n",
    "In this notebook, you'll learn how to use Pydantic models to define tools for OpenAI's tool calling API. You'll see how to reuse your existing models to create robust, validated tool definitions, and how to handle tool calls in your Python code. This lesson builds on your `UserInput` and `CustomerQuery` models from previous lessons.\n",
    "\n",
    "By the end of this lesson, you'll be able to:\n",
    "- Use Pydantic models to define tool schemas for OpenAI's tool calling API\n",
    "- Register your tool with the API using a validated schema\n",
    "- Handle tool calls and validate arguments with Pydantic\n",
    "- Integrate LLM-driven workflows with your own Python functions and data sources\n",
    "\n",
    "---\n",
    "\n",
    "### Import all required libraries and set up your environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77381ddd",
   "metadata": {
    "height": 234
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from pydantic import BaseModel, Field, EmailStr, field_validator\n",
    "from pydantic_ai import Agent\n",
    "from typing import Literal, List, Optional\n",
    "from datetime import datetime, date\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "import instructor\n",
    "from dotenv import load_dotenv\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06634afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mdavala/Desktop/MacbookPro2023Backup/Personal/Projects/Artificial_Intelligence/deeplearning.ai/Pydantic_for_LLM_Workflows\n",
      "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\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "load_dotenv()\n",
    "print(os.getcwd())\n",
    "# Print one variable (replace with your key name)\n",
    "print(os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b430dce",
   "metadata": {},
   "source": [
    "### Define your Pydantic models for user input and LLM output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "625c9ca5",
   "metadata": {
    "height": 438
   },
   "outputs": [],
   "source": [
    "# Define your UserInput model\n",
    "class UserInput(BaseModel):\n",
    "    name: str = Field(..., description=\"User's name\")\n",
    "    email: EmailStr = Field(..., description=\"User's email address\")\n",
    "    query: str = Field(..., description=\"User's query\")\n",
    "    order_id: Optional[str] = Field(\n",
    "        None,\n",
    "        description=\"Order ID if available (format: ABC-12345)\"\n",
    "    )\n",
    "    # Validate order_id format (e.g., ABC-12345)\n",
    "    @field_validator(\"order_id\")\n",
    "    def validate_order_id(cls, order_id):\n",
    "        import re\n",
    "        if order_id is None:\n",
    "            return order_id\n",
    "        pattern = r\"^[A-Z]{3}-\\d{5}$\"\n",
    "        if not re.match(pattern, order_id):\n",
    "            raise ValueError(\n",
    "                \"order_id must be in format ABC-12345 \"\n",
    "                \"(3 uppercase letters, dash, 5 digits)\"\n",
    "            )\n",
    "        return order_id\n",
    "    purchase_date: Optional[date] = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edfc3d84",
   "metadata": {
    "height": 217
   },
   "outputs": [],
   "source": [
    "# Define your CustomerQuery model\n",
    "class CustomerQuery(UserInput):\n",
    "    priority: str = Field(\n",
    "        ..., description=\"Priority level: low, medium, high\"\n",
    "    )\n",
    "    category: Literal[\n",
    "        'refund_request', 'information_request', 'other'\n",
    "    ] = Field(..., description=\"Query category\")\n",
    "    is_complaint: bool = Field(\n",
    "        ..., description=\"Whether this is a complaint\"\n",
    "    )\n",
    "    tags: List[str] = Field(..., description=\"Relevant keyword tags\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddeeff10",
   "metadata": {},
   "source": [
    "### Validate user input and create a CustomerQuery instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d808ad8",
   "metadata": {
    "height": 234
   },
   "outputs": [],
   "source": [
    "# Define a function to validate user input\n",
    "def validate_user_input(user_json: str):\n",
    "    \"\"\"Validate user input from a JSON string and return a UserInput \n",
    "    instance if valid.\"\"\"\n",
    "    try:\n",
    "        user_input = (\n",
    "            UserInput.model_validate_json(user_json)\n",
    "        )\n",
    "        print(\"user input validated...\")\n",
    "        return user_input\n",
    "    except Exception as e:\n",
    "        print(f\" Unexpected error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc275d7c",
   "metadata": {
    "height": 166
   },
   "outputs": [],
   "source": [
    "# Define a function to call an LLM using Pydantic AI to create an instance of CustomerQuery\n",
    "def create_customer_query(valid_user_json: str) -> CustomerQuery:\n",
    "    customer_query_agent = Agent(\n",
    "        #model=\"google-gla:gemini-2.0-flash\",\n",
    "        model=\"openai:gpt-4o-mini\",\n",
    "        output_type=CustomerQuery,\n",
    "    )\n",
    "    response = customer_query_agent.run_sync(valid_user_json)\n",
    "    print(\"CustomerQuery generated...\")\n",
    "    return response.output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e54f05d",
   "metadata": {},
   "source": [
    "### Try out your validation and query creation with sample input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d61c9ab",
   "metadata": {
    "height": 268
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user input validated...\n"
     ]
    },
    {
     "ename": "ModelHTTPError",
     "evalue": "status_code: 401, model_name: gpt-4o-mini, body: {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************EgAA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/MacbookPro2023Backup/Personal/Projects/Artificial_Intelligence/deeplearning.ai/.venv.deeplearning.ai/lib/python3.10/site-packages/pydantic_ai/models/openai.py:422\u001b[0m, in \u001b[0;36mOpenAIChatModel._completions_create\u001b[0;34m(self, messages, stream, model_settings, model_request_parameters)\u001b[0m\n\u001b[1;32m    421\u001b[0m     extra_headers\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m'\u001b[39m, get_user_agent())\n\u001b[0;32m--> 422\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m    423\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_name,\n\u001b[1;32m    424\u001b[0m         messages\u001b[38;5;241m=\u001b[39mopenai_messages,\n\u001b[1;32m    425\u001b[0m         parallel_tool_calls\u001b[38;5;241m=\u001b[39mmodel_settings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m'\u001b[39m, NOT_GIVEN),\n\u001b[1;32m    426\u001b[0m         tools\u001b[38;5;241m=\u001b[39mtools \u001b[38;5;129;01mor\u001b[39;00m NOT_GIVEN,\n\u001b[1;32m    427\u001b[0m         tool_choice\u001b[38;5;241m=\u001b[39mtool_choice \u001b[38;5;129;01mor\u001b[39;00m NOT_GIVEN,\n\u001b[1;32m    428\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    429\u001b[0m         stream_options\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minclude_usage\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m} \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m NOT_GIVEN,\n\u001b[1;32m    430\u001b[0m         stop\u001b[38;5;241m=\u001b[39mmodel_settings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstop_sequences\u001b[39m\u001b[38;5;124m'\u001b[39m, NOT_GIVEN),\n\u001b[1;32m    431\u001b[0m         max_completion_tokens\u001b[38;5;241m=\u001b[39mmodel_settings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m, NOT_GIVEN),\n\u001b[1;32m    432\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mmodel_settings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m, NOT_GIVEN),\n\u001b[1;32m    433\u001b[0m         response_format\u001b[38;5;241m=\u001b[39mresponse_format \u001b[38;5;129;01mor\u001b[39;00m NOT_GIVEN,\n\u001b[1;32m    434\u001b[0m         seed\u001b[38;5;241m=\u001b[39mmodel_settings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m'\u001b[39m, NOT_GIVEN),\n\u001b[1;32m    435\u001b[0m         reasoning_effort\u001b[38;5;241m=\u001b[39mmodel_settings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenai_reasoning_effort\u001b[39m\u001b[38;5;124m'\u001b[39m, NOT_GIVEN),\n\u001b[1;32m    436\u001b[0m         user\u001b[38;5;241m=\u001b[39mmodel_settings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenai_user\u001b[39m\u001b[38;5;124m'\u001b[39m, NOT_GIVEN),\n\u001b[1;32m    437\u001b[0m         web_search_options\u001b[38;5;241m=\u001b[39mweb_search_options \u001b[38;5;129;01mor\u001b[39;00m NOT_GIVEN,\n\u001b[1;32m    438\u001b[0m         service_tier\u001b[38;5;241m=\u001b[39mmodel_settings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenai_service_tier\u001b[39m\u001b[38;5;124m'\u001b[39m, NOT_GIVEN),\n\u001b[1;32m    439\u001b[0m         prediction\u001b[38;5;241m=\u001b[39mmodel_settings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenai_prediction\u001b[39m\u001b[38;5;124m'\u001b[39m, NOT_GIVEN),\n\u001b[1;32m    440\u001b[0m         temperature\u001b[38;5;241m=\u001b[39msampling_settings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m'\u001b[39m, NOT_GIVEN),\n\u001b[1;32m    441\u001b[0m         top_p\u001b[38;5;241m=\u001b[39msampling_settings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m'\u001b[39m, NOT_GIVEN),\n\u001b[1;32m    442\u001b[0m         presence_penalty\u001b[38;5;241m=\u001b[39msampling_settings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m'\u001b[39m, NOT_GIVEN),\n\u001b[1;32m    443\u001b[0m         frequency_penalty\u001b[38;5;241m=\u001b[39msampling_settings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m'\u001b[39m, NOT_GIVEN),\n\u001b[1;32m    444\u001b[0m         logit_bias\u001b[38;5;241m=\u001b[39msampling_settings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m'\u001b[39m, NOT_GIVEN),\n\u001b[1;32m    445\u001b[0m         logprobs\u001b[38;5;241m=\u001b[39msampling_settings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenai_logprobs\u001b[39m\u001b[38;5;124m'\u001b[39m, NOT_GIVEN),\n\u001b[1;32m    446\u001b[0m         top_logprobs\u001b[38;5;241m=\u001b[39msampling_settings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenai_top_logprobs\u001b[39m\u001b[38;5;124m'\u001b[39m, NOT_GIVEN),\n\u001b[1;32m    447\u001b[0m         extra_headers\u001b[38;5;241m=\u001b[39mextra_headers,\n\u001b[1;32m    448\u001b[0m         extra_body\u001b[38;5;241m=\u001b[39mmodel_settings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mextra_body\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m    449\u001b[0m     )\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m APIStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Desktop/MacbookPro2023Backup/Personal/Projects/Artificial_Intelligence/deeplearning.ai/.venv.deeplearning.ai/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:2583\u001b[0m, in \u001b[0;36mAsyncCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   2582\u001b[0m validate_response_format(response_format)\n\u001b[0;32m-> 2583\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m   2584\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2585\u001b[0m     body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[1;32m   2586\u001b[0m         {\n\u001b[1;32m   2587\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m   2588\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m   2589\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m: audio,\n\u001b[1;32m   2590\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m   2591\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m   2592\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m   2593\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m   2594\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m   2595\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[1;32m   2596\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m   2597\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m   2598\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodalities\u001b[39m\u001b[38;5;124m\"\u001b[39m: modalities,\n\u001b[1;32m   2599\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m   2600\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[1;32m   2601\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m: prediction,\n\u001b[1;32m   2602\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m   2603\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_cache_key\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt_cache_key,\n\u001b[1;32m   2604\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_effort\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_effort,\n\u001b[1;32m   2605\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[1;32m   2606\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msafety_identifier\u001b[39m\u001b[38;5;124m\"\u001b[39m: safety_identifier,\n\u001b[1;32m   2607\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m   2608\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[1;32m   2609\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m   2610\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[1;32m   2611\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m   2612\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[1;32m   2613\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m   2614\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m   2615\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m   2616\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m   2617\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m   2618\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m   2619\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverbosity\u001b[39m\u001b[38;5;124m\"\u001b[39m: verbosity,\n\u001b[1;32m   2620\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweb_search_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: web_search_options,\n\u001b[1;32m   2621\u001b[0m         },\n\u001b[1;32m   2622\u001b[0m         completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParamsStreaming\n\u001b[1;32m   2623\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[1;32m   2624\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParamsNonStreaming,\n\u001b[1;32m   2625\u001b[0m     ),\n\u001b[1;32m   2626\u001b[0m     options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m   2627\u001b[0m         extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m   2628\u001b[0m     ),\n\u001b[1;32m   2629\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[1;32m   2630\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   2631\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mAsyncStream[ChatCompletionChunk],\n\u001b[1;32m   2632\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/MacbookPro2023Backup/Personal/Projects/Artificial_Intelligence/deeplearning.ai/.venv.deeplearning.ai/lib/python3.10/site-packages/openai/_base_client.py:1794\u001b[0m, in \u001b[0;36mAsyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1791\u001b[0m opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1792\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1793\u001b[0m )\n\u001b[0;32m-> 1794\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls)\n",
      "File \u001b[0;32m~/Desktop/MacbookPro2023Backup/Personal/Projects/Artificial_Intelligence/deeplearning.ai/.venv.deeplearning.ai/lib/python3.10/site-packages/openai/_base_client.py:1594\u001b[0m, in \u001b[0;36mAsyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1593\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1594\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1596\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mAuthenticationError\u001b[0m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************EgAA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mModelHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Validate user input and create a CustomerQuery\u001b[39;00m\n\u001b[1;32m     12\u001b[0m valid_data \u001b[38;5;241m=\u001b[39m validate_user_input(user_input_json)\u001b[38;5;241m.\u001b[39mmodel_dump_json()\n\u001b[0;32m---> 13\u001b[0m customer_query \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_customer_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalid_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(customer_query))\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(customer_query\u001b[38;5;241m.\u001b[39mmodel_dump_json(indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n",
      "Cell \u001b[0;32mIn[11], line 8\u001b[0m, in \u001b[0;36mcreate_customer_query\u001b[0;34m(valid_user_json)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate_customer_query\u001b[39m(valid_user_json: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CustomerQuery:\n\u001b[1;32m      3\u001b[0m     customer_query_agent \u001b[38;5;241m=\u001b[39m Agent(\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;66;03m#model=\"google-gla:gemini-2.0-flash\",\u001b[39;00m\n\u001b[1;32m      5\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai:gpt-4o-mini\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m         output_type\u001b[38;5;241m=\u001b[39mCustomerQuery,\n\u001b[1;32m      7\u001b[0m     )\n\u001b[0;32m----> 8\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mcustomer_query_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_sync\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalid_user_json\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCustomerQuery generated...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39moutput\n",
      "File \u001b[0;32m~/Desktop/MacbookPro2023Backup/Personal/Projects/Artificial_Intelligence/deeplearning.ai/.venv.deeplearning.ai/lib/python3.10/site-packages/pydantic_ai/agent/abstract.py:306\u001b[0m, in \u001b[0;36mAbstractAgent.run_sync\u001b[0;34m(self, user_prompt, output_type, message_history, model, deps, model_settings, usage_limits, usage, infer_name, toolsets, event_stream_handler)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m infer_name \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_infer_name(inspect\u001b[38;5;241m.\u001b[39mcurrentframe())\n\u001b[0;32m--> 306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_event_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessage_history\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage_history\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43musage_limits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musage_limits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43musage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43minfer_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoolsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoolsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevent_stream_handler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevent_stream_handler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/MacbookPro2023Backup/Personal/Projects/Artificial_Intelligence/deeplearning.ai/.venv.deeplearning.ai/lib/python3.10/site-packages/nest_asyncio.py:98\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/asyncio/futures.py:201\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/asyncio/tasks.py:232\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    230\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    234\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "File \u001b[0;32m~/Desktop/MacbookPro2023Backup/Personal/Projects/Artificial_Intelligence/deeplearning.ai/.venv.deeplearning.ai/lib/python3.10/site-packages/pydantic_ai/agent/abstract.py:211\u001b[0m, in \u001b[0;36mAbstractAgent.run\u001b[0;34m(self, user_prompt, output_type, message_history, model, deps, model_settings, usage_limits, usage, infer_name, toolsets, event_stream_handler)\u001b[0m\n\u001b[1;32m    198\u001b[0m event_stream_handler \u001b[38;5;241m=\u001b[39m event_stream_handler \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent_stream_handler\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter(\n\u001b[1;32m    201\u001b[0m     user_prompt\u001b[38;5;241m=\u001b[39muser_prompt,\n\u001b[1;32m    202\u001b[0m     output_type\u001b[38;5;241m=\u001b[39moutput_type,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    209\u001b[0m     toolsets\u001b[38;5;241m=\u001b[39mtoolsets,\n\u001b[1;32m    210\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m agent_run:\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m agent_run:\n\u001b[1;32m    212\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m event_stream_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m    213\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_model_request_node(node) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_call_tools_node(node)\n\u001b[1;32m    214\u001b[0m         ):\n\u001b[1;32m    215\u001b[0m             \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m node\u001b[38;5;241m.\u001b[39mstream(agent_run\u001b[38;5;241m.\u001b[39mctx) \u001b[38;5;28;01mas\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/Desktop/MacbookPro2023Backup/Personal/Projects/Artificial_Intelligence/deeplearning.ai/.venv.deeplearning.ai/lib/python3.10/site-packages/pydantic_ai/run.py:149\u001b[0m, in \u001b[0;36mAgentRun.__anext__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__anext__\u001b[39m(\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    147\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _agent_graph\u001b[38;5;241m.\u001b[39mAgentNode[AgentDepsT, OutputDataT] \u001b[38;5;241m|\u001b[39m End[FinalResult[OutputDataT]]:\n\u001b[1;32m    148\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Advance to the next node automatically based on the last returned node.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 149\u001b[0m     next_node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph_run\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__anext__\u001b[39m()\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _agent_graph\u001b[38;5;241m.\u001b[39mis_agent_node(node\u001b[38;5;241m=\u001b[39mnext_node):\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m next_node\n",
      "File \u001b[0;32m~/Desktop/MacbookPro2023Backup/Personal/Projects/Artificial_Intelligence/deeplearning.ai/.venv.deeplearning.ai/lib/python3.10/site-packages/pydantic_graph/graph.py:761\u001b[0m, in \u001b[0;36mGraphRun.__anext__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    758\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_node, End):\n\u001b[1;32m    759\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopAsyncIteration\u001b[39;00m\n\u001b[0;32m--> 761\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_node)\n",
      "File \u001b[0;32m~/Desktop/MacbookPro2023Backup/Personal/Projects/Artificial_Intelligence/deeplearning.ai/.venv.deeplearning.ai/lib/python3.10/site-packages/pydantic_graph/graph.py:734\u001b[0m, in \u001b[0;36mGraphRun.next\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpersistence\u001b[38;5;241m.\u001b[39mrecord_run(node_snapshot_id):\n\u001b[1;32m    733\u001b[0m         ctx \u001b[38;5;241m=\u001b[39m GraphRunContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeps)\n\u001b[0;32m--> 734\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m node\u001b[38;5;241m.\u001b[39mrun(ctx)\n\u001b[1;32m    736\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_node, End):\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_snapshot_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_node\u001b[38;5;241m.\u001b[39mget_snapshot_id()\n",
      "File \u001b[0;32m~/Desktop/MacbookPro2023Backup/Personal/Projects/Artificial_Intelligence/deeplearning.ai/.venv.deeplearning.ai/lib/python3.10/site-packages/pydantic_ai/_agent_graph.py:297\u001b[0m, in \u001b[0;36mModelRequestNode.run\u001b[0;34m(self, ctx)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_did_stream:\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;66;03m# `self._result` gets set when exiting the `stream` contextmanager, so hitting this\u001b[39;00m\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;66;03m# means that the stream was started but not finished before `run()` was called\u001b[39;00m\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mAgentRunError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYou must finish streaming before calling run()\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[0;32m--> 297\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(ctx)\n",
      "File \u001b[0;32m~/Desktop/MacbookPro2023Backup/Personal/Projects/Artificial_Intelligence/deeplearning.ai/.venv.deeplearning.ai/lib/python3.10/site-packages/pydantic_ai/_agent_graph.py:339\u001b[0m, in \u001b[0;36mModelRequestNode._make_request\u001b[0;34m(self, ctx)\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m    338\u001b[0m model_settings, model_request_parameters, message_history, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_request(ctx)\n\u001b[0;32m--> 339\u001b[0m model_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m ctx\u001b[38;5;241m.\u001b[39mdeps\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mrequest(message_history, model_settings, model_request_parameters)\n\u001b[1;32m    340\u001b[0m ctx\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39musage\u001b[38;5;241m.\u001b[39mrequests \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_finish_handling(ctx, model_response)\n",
      "File \u001b[0;32m~/Desktop/MacbookPro2023Backup/Personal/Projects/Artificial_Intelligence/deeplearning.ai/.venv.deeplearning.ai/lib/python3.10/site-packages/pydantic_ai/models/openai.py:337\u001b[0m, in \u001b[0;36mOpenAIChatModel.request\u001b[0;34m(self, messages, model_settings, model_request_parameters)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    332\u001b[0m     messages: \u001b[38;5;28mlist\u001b[39m[ModelMessage],\n\u001b[1;32m    333\u001b[0m     model_settings: ModelSettings \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    334\u001b[0m     model_request_parameters: ModelRequestParameters,\n\u001b[1;32m    335\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ModelResponse:\n\u001b[1;32m    336\u001b[0m     check_allow_model_requests()\n\u001b[0;32m--> 337\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_completions_create(\n\u001b[1;32m    338\u001b[0m         messages, \u001b[38;5;28;01mFalse\u001b[39;00m, cast(OpenAIChatModelSettings, model_settings \u001b[38;5;129;01mor\u001b[39;00m {}), model_request_parameters\n\u001b[1;32m    339\u001b[0m     )\n\u001b[1;32m    340\u001b[0m     model_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(response)\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_response\n",
      "File \u001b[0;32m~/Desktop/MacbookPro2023Backup/Personal/Projects/Artificial_Intelligence/deeplearning.ai/.venv.deeplearning.ai/lib/python3.10/site-packages/pydantic_ai/models/openai.py:452\u001b[0m, in \u001b[0;36mOpenAIChatModel._completions_create\u001b[0;34m(self, messages, stream, model_settings, model_request_parameters)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m APIStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (status_code \u001b[38;5;241m:=\u001b[39m e\u001b[38;5;241m.\u001b[39mstatus_code) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m400\u001b[39m:\n\u001b[0;32m--> 452\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ModelHTTPError(status_code\u001b[38;5;241m=\u001b[39mstatus_code, model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name, body\u001b[38;5;241m=\u001b[39me\u001b[38;5;241m.\u001b[39mbody) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mModelHTTPError\u001b[0m: status_code: 401, model_name: gpt-4o-mini, body: {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************EgAA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}"
     ]
    }
   ],
   "source": [
    "# Define user input JSON data\n",
    "user_input_json = '''\n",
    "{\n",
    "    \"name\": \"Joe User\",\n",
    "    \"email\": \"joe@example.com\",\n",
    "    \"query\": \"When can I expect delivery of the headphones I ordered?\",\n",
    "    \"order_id\": \"ABC-12345\",\n",
    "    \"purchase_date\": \"2025-12-01\"\n",
    "}\n",
    "'''\n",
    "# Validate user input and create a CustomerQuery\n",
    "valid_data = validate_user_input(user_input_json).model_dump_json()\n",
    "customer_query = create_customer_query(valid_data)\n",
    "print(type(customer_query))\n",
    "print(customer_query.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8aa75d",
   "metadata": {},
   "source": [
    "### Define tool input models for FAQ lookup and order status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0808ce03",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "# Define FAQ Lookup tool input as a Pydantic model\n",
    "class FAQLookupArgs(BaseModel):\n",
    "    query: str = Field(..., description=\"User's query\") \n",
    "    tags: List[str] = Field(\n",
    "        ..., description=\"Relevant keyword tags from the customer query\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f48958",
   "metadata": {
    "height": 302
   },
   "outputs": [],
   "source": [
    "# Define Check Order Status tool input as a Pydantic model\n",
    "class CheckOrderStatusArgs(BaseModel):\n",
    "    order_id: str = Field(\n",
    "        ..., description=\"Customer's order ID (format: ABC-12345)\"\n",
    "    )\n",
    "    email: EmailStr = Field(..., description=\"Customer's email address\")\n",
    "\n",
    "    @field_validator(\"order_id\")\n",
    "    def validate_order_id(cls, order_id):\n",
    "        import re\n",
    "        pattern = r\"^[A-Z]{3}-\\d{5}$\"\n",
    "        if not re.match(pattern, order_id):\n",
    "            raise ValueError(\n",
    "                \"order_id must be in format ABC-12345 \"\n",
    "                \"(3 uppercase letters, dash, 5 digits)\"\n",
    "            )\n",
    "        return order_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc02b40",
   "metadata": {},
   "source": [
    "### Create example FAQ and order databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d727a804",
   "metadata": {
    "height": 676
   },
   "outputs": [],
   "source": [
    "# Create a fake FAQ database as a list of entries with keywords\n",
    "faq_db = [\n",
    "    {\n",
    "        \"question\": \"How can I reset my password?\",\n",
    "        \"answer\": \"To reset your password, click 'Forgot Password' on the sign-in page and follow the instructions sent to your email.\",\n",
    "        \"keywords\": [\"password\", \"reset\", \"account\"]\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How long does shipping take?\",\n",
    "        \"answer\": \"Standard shipping takes 3-5 business days. You can track your order in your account dashboard.\",\n",
    "        \"keywords\": [\"shipping\", \"delivery\", \"order\", \"tracking\"]\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How can I return an item?\",\n",
    "        \"answer\": \"You can return any item within 30 days of purchase. Visit our returns page to start the process.\",\n",
    "        \"keywords\": [\"return\", \"refund\", \"exchange\"]\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How can I delete my account?\",\n",
    "        \"answer\": \"To delete your account, go to your account settings tab and select 'delete account'.\",\n",
    "        \"keywords\": [\"delete\", \"account\", \"remove\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create a fake order database\n",
    "order_db = {\n",
    "    \"ABC-12345\": {\n",
    "        \"status\": \"shipped\", \"estimated_delivery\": \"2025-12-05\",\n",
    "        \"purchase_date\": \"2025-12-01\", \"email\": \"joe@example.com\"\n",
    "    },\n",
    "    \"XYZ-23456\": {\n",
    "        \"status\": \"processing\", \"estimated_delivery\": \"2025-12-15\",\n",
    "        \"purchase_date\": \"2025-12-10\", \"email\": \"sue@example.com\"\n",
    "    },\n",
    "    \"QWE-34567\": {\n",
    "        \"status\": \"delivered\", \"estimated_delivery\": \"2025-12-20\",\n",
    "        \"purchase_date\": \"2025-12-18\", \"email\": \"bob@example.com\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967410e5",
   "metadata": {},
   "source": [
    "### Implement tool functions for FAQ lookup and order status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3ec625",
   "metadata": {
    "height": 302
   },
   "outputs": [],
   "source": [
    "# Define your FAQ lookup tool\n",
    "def lookup_faq_answer(args: FAQLookupArgs) -> str:\n",
    "    \"\"\"Look up an FAQ answer by matching tags and words in query \n",
    "    to FAQ entry keywords.\"\"\"\n",
    "    query_words = set(word.lower() for word in args.query.split())\n",
    "    tag_set = set(tag.lower() for tag in args.tags)\n",
    "    best_match = None\n",
    "    best_score = 0\n",
    "    for faq in faq_db:\n",
    "        keywords = set(k.lower() for k in faq[\"keywords\"])\n",
    "        score = len(keywords & tag_set) + len(keywords & query_words)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_match = faq\n",
    "    if best_match and best_score > 0:\n",
    "        return best_match[\"answer\"]\n",
    "    return \"Sorry, I couldn't find an FAQ answer for your question.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad844661",
   "metadata": {
    "height": 438
   },
   "outputs": [],
   "source": [
    "# Define your check order status tool\n",
    "def check_order_status(args: CheckOrderStatusArgs):\n",
    "    \"\"\"Simulate checking the status of a customer's order by \n",
    "    order_id and email.\"\"\"\n",
    "    order = order_db.get(args.order_id)\n",
    "    if not order:\n",
    "        return {\n",
    "            \"order_id\": args.order_id,\n",
    "            \"status\": \"not found\",\n",
    "            \"estimated_delivery\": None,\n",
    "            \"note\": \"order_id not found\"\n",
    "        }\n",
    "    if args.email.lower() != order.get(\"email\", \"\").lower():\n",
    "        return {\n",
    "            \"order_id\": args.order_id,\n",
    "            \"status\": order[\"status\"],\n",
    "            \"estimated_delivery\": order[\"estimated_delivery\"],\n",
    "            \"note\": \"order_id found but email mismatch\"\n",
    "        }\n",
    "    return {\n",
    "        \"order_id\": args.order_id,\n",
    "        \"status\": order[\"status\"],\n",
    "        \"estimated_delivery\": order[\"estimated_delivery\"],\n",
    "        \"note\": \"order_id and email match\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8691a12c",
   "metadata": {},
   "source": [
    "### Define tool schemas for OpenAI tool calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64829f3",
   "metadata": {
    "height": 336
   },
   "outputs": [],
   "source": [
    "# Define tools for your API call\n",
    "tool_definitions = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"lookup_faq_answer\",\n",
    "            \"description\": \"Look up an FAQ answer by matching tags to FAQ entry keywords.\",\n",
    "            \"parameters\": FAQLookupArgs.model_json_schema()\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"check_order_status\",\n",
    "            \"description\": \"Check the status of a customer's order.\",\n",
    "            \"parameters\": CheckOrderStatusArgs.model_json_schema()\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3b9825",
   "metadata": {},
   "source": [
    "### Define your support ticket output model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9590e3",
   "metadata": {
    "height": 387
   },
   "outputs": [],
   "source": [
    "#Define your final output Pydantic models\n",
    "class OrderDetails(BaseModel):\n",
    "    status: str\n",
    "    estimated_delivery: str\n",
    "    note: str\n",
    "\n",
    "class SupportTicket(CustomerQuery):\n",
    "    recommended_next_action: Literal[\n",
    "        'escalate_to_agent', 'send_faq_response', \n",
    "        'send_order_status', 'no_action_needed'\n",
    "    ] = Field(\n",
    "        ..., description=\"LLM's recommended next action for support\"\n",
    "    )\n",
    "    order_details: Optional[OrderDetails] = Field(\n",
    "        None, description=\"Order details if action is send_order_status\"\n",
    "    )\n",
    "    faq_response: Optional[str] = Field(\n",
    "        None, description=\"FAQ response if action is send_faq_response\"\n",
    "    )\n",
    "    creation_date: datetime = Field(\n",
    "        ..., description=\"Date and time the ticket was created\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f60555d",
   "metadata": {},
   "source": [
    "### Decide on the next support action using OpenAI tool calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d5bde1",
   "metadata": {
    "height": 642
   },
   "outputs": [],
   "source": [
    "# Initialize OpenAI client\n",
    "client = OpenAI()\n",
    "\n",
    "# Define a function to call OpenAI with tools\n",
    "def decide_next_action_with_tools(customer_query: CustomerQuery):\n",
    "    \n",
    "    support_ticket_schema = json.dumps(\n",
    "        SupportTicket.model_json_schema(), indent=2\n",
    "    )\n",
    "    system_prompt = f\"\"\"\n",
    "        You are a helpful customer support agent. Your job is to \n",
    "        determine what support action should be taken for the customer, \n",
    "        based on the customer query and the expected fields in the \n",
    "        SupportTicket schema below. If more information on a particular \n",
    "        order_id or FAQ response would be helpful in responding to the \n",
    "        user query and can be obtained by calling a tool, call the \n",
    "        appropriate tool to get that information. If an order_id is \n",
    "        present in the query, always look up the order status to get \n",
    "        more information on the order.\n",
    "\n",
    "        Here is the JSON schema for the SupportTicket model you must \n",
    "        use as context for what information is expected:\n",
    "        {support_ticket_schema}\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": str(customer_query.model_dump())}\n",
    "    ]\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=messages,\n",
    "        tools=tool_definitions,\n",
    "        tool_choice=\"auto\"\n",
    "    )\n",
    "    message = response.choices[0].message\n",
    "    tool_calls = getattr(message, \"tool_calls\", None)\n",
    "    return message, tool_calls, messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ea3a33",
   "metadata": {},
   "source": [
    "### Inspect the LLM's outputs and tool calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdbda29",
   "metadata": {
    "height": 183
   },
   "outputs": [],
   "source": [
    "# Call the decide_next_action_with_tools function\n",
    "message, tool_calls, messages = decide_next_action_with_tools(\n",
    "    customer_query\n",
    ")\n",
    "# Investigate the LLM's outputs before proceeding\n",
    "print(\"LLM message:\\n\", json.dumps(message.model_dump(), indent=2))\n",
    "print(\n",
    "    \"\\nTool calls:\\n\", \n",
    "    json.dumps([call.model_dump() for call in tool_calls], indent=2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5755107c",
   "metadata": {},
   "source": [
    "### Gather tool outputs and prepare for ticket generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede983e8",
   "metadata": {
    "height": 540
   },
   "outputs": [],
   "source": [
    "# Define a function to get tool outputs\n",
    "def get_tool_outputs(tool_calls):\n",
    "    tool_outputs = []\n",
    "    if tool_calls:\n",
    "        for tool_call in tool_calls:\n",
    "            if tool_call.function.name == \"lookup_faq_answer\":\n",
    "                print(\"Agent requested a call to the Lookup FAQ tool...\")\n",
    "                args = FAQLookupArgs.model_validate_json(\n",
    "                    tool_call.function.arguments\n",
    "                )\n",
    "                result = lookup_faq_answer(args)\n",
    "                tool_outputs.append({\n",
    "                    \"tool_call_id\": tool_call.id, \"output\": result\n",
    "                })\n",
    "                print(f\"Lookup FAQ tool returned {result}\")\n",
    "            elif tool_call.function.name == \"check_order_status\":\n",
    "                print(\"Agent requested a call to Check Order Status tool...\")\n",
    "                args = CheckOrderStatusArgs.model_validate_json(\n",
    "                    tool_call.function.arguments\n",
    "                )\n",
    "                result = check_order_status(args)\n",
    "                tool_outputs.append({\n",
    "                    \"tool_call_id\": tool_call.id, \"output\": result\n",
    "                })\n",
    "                print(f\"Check Order Status tool returned {result}\")\n",
    "    return tool_outputs\n",
    "\n",
    "tool_outputs = get_tool_outputs(tool_calls)\n",
    "\n",
    "# Print tool outputs for inspection\n",
    "print(\"Tool outputs:\\n\", json.dumps(tool_outputs, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533e57d8",
   "metadata": {},
   "source": [
    "### Generate a structured support ticket using Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffe1204",
   "metadata": {
    "height": 642
   },
   "outputs": [],
   "source": [
    "# Create the Anthropic client with Instructor\n",
    "anthropic_client = instructor.from_anthropic(\n",
    "    anthropic.Anthropic()\n",
    ")\n",
    "\n",
    "# Define a function to call Anthropic to generate a support ticket\n",
    "def generate_structured_support_ticket(\n",
    "    customer_query: CustomerQuery, message, tool_outputs: list\n",
    "):\n",
    "    tool_results_str = \"\\n\".join([\n",
    "        f\"Tool: {out['tool_call_id']} Output: {json.dumps(out['output'])}\"\n",
    "        for out in tool_outputs\n",
    "    ]) if tool_outputs else \"No tool calls were made.\"\n",
    "    # Concatenate prompt parts into a single string for Anthropic\n",
    "    prompt = f\"\"\"\n",
    "        You are a support agent. Use all information below to \n",
    "        generate a support ticket as a validated Pydantic model.\n",
    "        Customer query: {customer_query.model_dump_json(indent=2)}\n",
    "        LLM message: {str(message.content)}\n",
    "        Tool results: {tool_results_str}\n",
    "    \"\"\"\n",
    "    # Create the message with structured output\n",
    "    response = anthropic_client.messages.create(\n",
    "        model=\"claude-3-7-sonnet-latest\",  \n",
    "        max_tokens=1024,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        response_model=SupportTicket\n",
    "    )\n",
    "    \n",
    "    support_ticket = response\n",
    "    support_ticket.creation_date = datetime.now()\n",
    "    return support_ticket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36fa025",
   "metadata": {},
   "source": [
    "### Print your final support ticket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9178b90e",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "# Run the final step of generating a support ticket and print output\n",
    "support_ticket = generate_structured_support_ticket(\n",
    "    customer_query, message, tool_outputs\n",
    ")\n",
    "print(support_ticket.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b8c9c8",
   "metadata": {},
   "source": [
    "### Full workflow: validate, query, decide, tool, and generate ticket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c3226e",
   "metadata": {
    "height": 183
   },
   "outputs": [],
   "source": [
    "# Define new user input data\n",
    "user_json = '''\n",
    "{\n",
    "    \"name\": \"Joe User\",\n",
    "    \"email\": \"joe@example.com\",\n",
    "    \"query\": \"I'm really not happy with this product I bought\",\n",
    "    \"order_id\": \"QWE-34567\",\n",
    "    \"purchase_date\": null\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc970a73",
   "metadata": {
    "height": 200
   },
   "outputs": [],
   "source": [
    "# Run the entire pipeline\n",
    "valid_user_json = validate_user_input(user_json).model_dump_json()\n",
    "customer_query = create_customer_query(valid_user_json)\n",
    "message, tool_calls, messages = decide_next_action_with_tools(\n",
    "    customer_query\n",
    ")\n",
    "tool_outputs = get_tool_outputs(tool_calls)\n",
    "support_ticket = generate_structured_support_ticket(\n",
    "    customer_query, message, tool_outputs\n",
    ")\n",
    "print(support_ticket.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec150c7b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this lesson, you learned how to use Pydantic models to define tools for OpenAI's tool calling API and build sophisticated customer support workflows. You saw how to reuse your existing models to create robust, validated tool definitions, handle tool calls in your Python code, and automatically gather information to generate comprehensive support tickets. This approach demonstrates the power of Pydantic for creating end-to-end validated workflows with LLMs, from input validation to tool definitions to structured output generation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv.deeplearning.ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
